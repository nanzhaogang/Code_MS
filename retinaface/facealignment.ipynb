{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# FaceAlignment-2D人脸对齐案例"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "本案例将基于Helen数据集讲述如何在mindspore中进行2d人脸对齐"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1 准备环节"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1 导入模块"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "导入模块需要用到部分src中的文件，校验时请保持该notebook与src文件夹平级。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "import scipy.io as scio\n",
    "from typing import List\n",
    "\n",
    "import mindspore as ms\n",
    "import mindspore.nn as nn\n",
    "import mindspore.dataset as ds\n",
    "from mindspore import load_checkpoint, load_param_into_net, Tensor\n",
    "from mindspore.mindrecord import FileWriter\n",
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, Callback"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2 环境配置"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "使用GRAPH模式进行实验并使用GPU环境。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "ms.set_context(mode=ms.GRAPH_MODE, device_target='GPU', save_graphs=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3 数据集准备"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.3.1 下载数据集"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "下载案例所用到的[人脸对齐数据集](http://www.ifp.illinois.edu/~vuongle2/helen/)，该数据集包含2,330个图像，其中，2,000个图像位于训练集，330个位于验证集，每一张图像都有194个关键点的标注。网页中给出了一些下载链接，包括训练（Train images）和测试用图像（Test images）还有标注（Annotation）,我们需要在网页中下载上述数据。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.3.2 下载Bounding Box标注"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "下载案例所用到的[BoundingBox标注](https://ibug.doc.ic.ac.uk/media/uploads/competitions/bounding_boxes.zip)。该链接指向的文件包含bounding_boxes_helen_trainset.mat和bounding_boxes_helen_testset.mat，其中含有对每个人脸的BoundingBox标注，可以用于对原始照片的裁剪。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.3.3 下载完成后需要将数据集目录构建成如下形式\n",
    "\n",
    "```text\n",
    "├── Helen/\n",
    "    ├── annotation/\n",
    "        ├── 1.txt\n",
    "        ├── 2.txt\n",
    "        ├── 3.txt\n",
    "        ├── ...\n",
    "    ├── train/\n",
    "        ├── 232194_1.jpg\n",
    "        ├── 1629243_1.jpg\n",
    "        ├── 1681766_1.jpg\n",
    "        ├── ...\n",
    "    ├── bounding_boxes_helen_trainset.mat\n",
    "    ├── trainname.txt\n",
    "```\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.3.4 我不知道如何构建数据集"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "你可以从这个链接获取我已经按照上述结构整理好的数据集: https://pan.baidu.com/s/1rFjm2BEL1F9N-y3MMs2o3Q     (访问密码 hele)\n",
    "里面有Helen_192及其db文件，以及有Helen.zip。Helen.zip解压得到上述目录结构，而Helen_192是整理好的mindrecord文件(裁剪，无数据增强)，如果使用这个mindrecord文件可以跳过数据集制作过程。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. 处理数据"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 在py文件中定义参数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "数据集相关处理参数如下：\n",
    "    img_size:将图片统一处理到的边长。\n",
    "    dataset_side_data_enhance:数据集侧的数据增强，主要采用旋转方法对图片进行四向旋转，启用后会同时旋转标记而不是简单的仅处理图片。\n",
    "    dataset_target_path:输出文件名，任意修改，但是当文件夹下有同名文件的时候，新的数据集将不会创建且不会覆盖。\n",
    "    clip:是否裁剪。不裁剪的话不会使用boundingbox文件处理图片。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "dataset_config = {\n",
    "    'img_size': 192,\n",
    "    'dataset_side_data_enhance': 'False',\n",
    "    'dataset_target_path': 'Helen_192_no_enhance_do_clip',\n",
    "    'clip': True\n",
    "}# Helen Dataset\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "完整的配置文件如下所示："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "config = {\n",
    "\n",
    "    # Helen Dataset\n",
    "    'img_size': 192,\n",
    "    'dataset_side_data_enhance': 'False',\n",
    "    'dataset_target_path': 'Helen_192_no_enhance_do_clip',\n",
    "    'clip': True,\n",
    "\n",
    "    # FaceAlignment Config\n",
    "    'num_classes': 388,\n",
    "    'batch_size': 4,\n",
    "    'epoch_size': 1000,\n",
    "    'warmup_epochs': 4,\n",
    "    'lr': 0.0001,\n",
    "    'momentum': 0.9,\n",
    "    'weight_decay': 0.00004,\n",
    "    'loss_scale': 1024,\n",
    "    'save_checkpoint': True,\n",
    "    'save_checkpoint_epochs': 10,\n",
    "    'keep_checkpoint_max': 500,\n",
    "    'save_checkpoint_path': \"./checkpoint\",\n",
    "    'export_format': \"MINDIR\",\n",
    "    'export_file': \"FaceAlignment_2D\"\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 制作mindrecord数据集"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "这一步的主要原因是数据集处理时间较长，所以处理成mindrecord方便随时读取"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def to_mindrocord(img_size, output_path, clip, dataset_side_data_enhance=False):\n",
    "    \"\"\"\n",
    "    Write Helen Dataset to Mindrecord File\n",
    "\n",
    "    Args:\n",
    "        clip: Clip Picture or Not\n",
    "        img_size: Compress each img to [img_size, img_size, 3]\n",
    "        output_path(string): Output MindRecord File Path\n",
    "        dataset_side_data_enhance(bool): Rotate image with annotations or not. Default: False\n",
    "\n",
    "    Returns:\n",
    "        No Direct Return\n",
    "        But Generate mindrecord File With 2 Columns ['label', 'image'] at 'output_path'\n",
    "\n",
    "    Examples:\n",
    "        >>> to_mindrocord(192, '/mnt/Helen_192', True, True)\n",
    "    \"\"\"\n",
    "    finalpictures, annotations = read_helen(img_size, dataset_side_data_enhance, clip)\n",
    "\n",
    "    writer = FileWriter(file_name=output_path, shard_num=1)\n",
    "    cv_schema = {\"image\": {\"type\": \"float32\", \"shape\": [img_size, img_size, 3]},\n",
    "                 \"label\": {\"type\": \"float32\", \"shape\": [1, 388]}}\n",
    "    writer.add_schema(cv_schema, \"Face Alignment Dataset\")\n",
    "\n",
    "    data = []\n",
    "    limit = 8000 if dataset_side_data_enhance == 'True' else 2000\n",
    "    for i in range(limit):\n",
    "        sample = {}\n",
    "        sample['label'] = annotations[i]\n",
    "        sample['image'] = finalpictures[i]\n",
    "\n",
    "        data.append(sample)\n",
    "        if i % 10 == 0:\n",
    "            writer.write_raw_data(data)\n",
    "            data = []\n",
    "    if data:\n",
    "        writer.write_raw_data(data)\n",
    "    writer.commit()\n",
    "\n",
    "def read_helen(img_size, dataset_side_data_enhance=False, clip=False):\n",
    "    \"\"\"\n",
    "    Read Helen Data In Files and Generate Dataset\n",
    "\n",
    "    Args:\n",
    "        clip: Clip Picture or Not. Default: False.\n",
    "        img_size: Compress each img to [img_size, img_size, 3]\n",
    "        dataset_side_data_enhance: Rotate or not. Default: False\n",
    "\n",
    "    Returns:\n",
    "        finalpictures: Array, Contain multiple Pictures in [-1, img_size, img_size, 3]\n",
    "        annotations: Array, Contain multiple annotations in [-1, img_size, img_size, 3]\n",
    "\n",
    "    \"\"\"\n",
    "    filename = []\n",
    "    with open(\"src/process_datasets/Helen/trainname.txt\") as file:\n",
    "        for item in file:\n",
    "            filename.append(item.replace(\"\\n\", \"\"))\n",
    "    file.close()\n",
    "    root_dir = \"src/process_datasets/Helen/\"\n",
    "    bounding_box = scio.loadmat(root_dir + \"bounding_boxes_helen_trainset.mat\").get(\"bounding_boxes\")[0]\n",
    "\n",
    "    groundtruthboxes = []\n",
    "    detectorboxes = []\n",
    "    finalpictures = []\n",
    "    annotations = []\n",
    "    for i in range(0, 2000):\n",
    "        assert str(filename[i] + \".jpg\") == bounding_box[i][0][0][0][0]\n",
    "\n",
    "        img_path = root_dir + \"train/\" + filename[i] + \".jpg\"\n",
    "        img = cv2.imread(img_path, flags=1)\n",
    "        annotation_path = root_dir + \"annotation/\" + str(i + 1) + \".txt\"\n",
    "        annotation = read_csv(annotation_path)\n",
    "        ground_truth_box = bounding_box[i][0][0][2][0].astype(np.int32)\n",
    "        groundtruthboxes.append(ground_truth_box)\n",
    "        detecter_box = bounding_box[i][0][0][1][0].astype(np.int32)\n",
    "        detectorboxes.append(detecter_box)\n",
    "        if clip:\n",
    "            final_pic = picture_clip(img, ground_truth_box)\n",
    "            final_pic, new_annotation = picture_resize(final_pic, annotation, ground_truth_box[0],\n",
    "                                                       ground_truth_box[1], img_size)\n",
    "        else:\n",
    "            final_pic = img\n",
    "            final_pic, new_annotation = picture_resize(final_pic, annotation, 0, 0,\n",
    "                                                       img_size)\n",
    "        final_pic = final_pic.astype(np.float32)\n",
    "        new_annotation = new_annotation.astype(np.float32)\n",
    "        if dataset_side_data_enhance == 'True':\n",
    "            pic_1 = cv2.rotate(final_pic, cv2.ROTATE_90_CLOCKWISE)\n",
    "            anno_1 = new_annotation.copy()\n",
    "            anno_1[:, 0] = img_size - new_annotation[:, 1]\n",
    "            anno_1[:, 1] = new_annotation[:, 0].copy()\n",
    "            pic_2 = cv2.rotate(final_pic, cv2.ROTATE_180)\n",
    "            anno_2 = new_annotation.copy()\n",
    "            anno_2[:, 0] = img_size - new_annotation[:, 0]\n",
    "            anno_2[:, 1] = img_size - new_annotation[:, 1]\n",
    "            pic_3 = cv2.rotate(final_pic, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "            anno_3 = new_annotation.copy()\n",
    "            anno_3[:, 0] = new_annotation[:, 1].copy()\n",
    "            anno_3[:, 1] = img_size - new_annotation[:, 0]\n",
    "            finalpictures.append(pic_1)\n",
    "            annotations.append(anno_1.astype(np.float32))\n",
    "            finalpictures.append(pic_2)\n",
    "            annotations.append(anno_2.astype(np.float32))\n",
    "            finalpictures.append(pic_3)\n",
    "            annotations.append(anno_3.astype(np.float32))\n",
    "        finalpictures.append(final_pic)\n",
    "        annotations.append(new_annotation.astype(np.float32))\n",
    "    return finalpictures, annotations\n",
    "\n",
    "def read_csv(path):\n",
    "    \"\"\"\n",
    "    Read csv File\n",
    "    Args :\n",
    "        path(str): Helen Annotation TXT File Path\n",
    "\n",
    "    Returns :\n",
    "        result(numpy.ndarray): Annotation Data in np.ndarray. For Helen Dataset, output shape is (194, 2).\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        for row in reader:\n",
    "            data.append(row)\n",
    "    result = np.array(data[1:], dtype=float)\n",
    "    return result\n",
    "\n",
    "def picture_clip(pic, box):\n",
    "    \"\"\"\n",
    "    Clip Image Using Bounding Box\n",
    "\n",
    "    Input :\n",
    "        pic(ndarray) : Picture at any size\n",
    "        box(ndarray) : Box in [xMin,yMin,xMax,yMax]\n",
    "\n",
    "    Output : Clipped Picture\n",
    "    Example :\n",
    "        >>> picture_clip(pic, [1, 5, 65, 97])\n",
    "    \"\"\"\n",
    "    xmin, ymin, xmax, ymax = box[0], box[1], box[2], box[3]\n",
    "    img_crop = pic[int(ymin):int(ymax), int(xmin):int(xmax)].copy()\n",
    "    return img_crop\n",
    "\n",
    "\n",
    "def picture_resize(picture, annotation, x0, y0, target_size):\n",
    "    \"\"\"\n",
    "    Resize Picture And Adjusy Annotation According to Start Point and Target Size\n",
    "    Pictures should be resized, annotations need sub and 'resize'\n",
    "\n",
    "    Input :\n",
    "        Picture : CV2 Picture [ W, H, C ]\n",
    "        annotation : Marked Points , Absolute Position , [(x1,y1),(x2,y2)...]\n",
    "        x0 : Bounding Box's Left Upper Corner's Position on X axis\n",
    "        y0 : Bounding Box's Left Upper Corner's Position on Y axis\n",
    "        target_size : Will Resize Image To (target_size, target_size)\n",
    "\n",
    "    Output :\n",
    "        Picture : Resized Picture\n",
    "        annotation ： annotations , But Relative Position , Relate to Resized Picture\n",
    "\n",
    "    Examples:\n",
    "        >>>picture_resize(img, annotation, 10, 20, 192)\n",
    "    \"\"\"\n",
    "    y_ratio, x_ratio = target_size / picture.shape[0], target_size / picture.shape[1]\n",
    "    img_resized = cv2.resize(picture, (target_size, target_size))\n",
    "    img_resized = img_resized / 255\n",
    "    annotation[:, 0] = annotation[:, 0] - x0\n",
    "    annotation[:, 1] = annotation[:, 1] - y0\n",
    "    annotation[:, 0] = annotation[:, 0] * x_ratio\n",
    "    annotation[:, 1] = annotation[:, 1] * y_ratio\n",
    "\n",
    "    return img_resized, annotation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# to_mindrocord(config['img_size'], config['dataset_target_path'], config['clip'], dataset_side_data_enhance=config['dataset_side_data_enhance'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "处理完毕后可以在同级目录下找到mindrecord及其db文件，加载数据集就可以简单的用MindDataset语句进行加载，要读取的列有两列，分别是image和label，num_parallel_workers和shuffle按照需求选择。读取后进行数据转换以及其他操作"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def data_load(mindrecord_path, do_train, batch_size=1, repeat_num=1, count_number=False, num_worker=4, shuffle=None):\n",
    "    dataset = ds.MindDataset(mindrecord_path, columns_list=[\"image\", \"label\"], num_parallel_workers=num_worker, shuffle=shuffle)\n",
    "    count = 0\n",
    "    if count_number:\n",
    "        print(\"Calculating Size\")\n",
    "        count = 0\n",
    "        for _ in dataset.create_dict_iterator(output_numpy=True):\n",
    "            # print(\"sample: {}\".format(item))\n",
    "            count += 1\n",
    "        print(\"Got {} samples in Total, Load Successful\".format(count))\n",
    "\n",
    "    buffer_size = 1000\n",
    "    normalize_op = ds.vision.c_transforms.Normalize(mean=[0.485 * 255, 0.456 * 255, 0.406 * 255],\n",
    "                                                    std=[0.229 * 255, 0.224 * 255, 0.225 * 255])\n",
    "    change_swap_op = ds.vision.c_transforms.HWC2CHW()\n",
    "    type_cast_op = ds.transforms.c_transforms.TypeCast(ms.float32)\n",
    "    if do_train:\n",
    "        trans = [normalize_op, change_swap_op, type_cast_op]\n",
    "        dataset = dataset.map(operations=trans, input_columns=\"image\", num_parallel_workers=num_worker)\n",
    "        dataset = dataset.map(operations=type_cast_op, input_columns=\"label\", num_parallel_workers=num_worker)\n",
    "    else:\n",
    "        trans = [normalize_op, change_swap_op, type_cast_op]\n",
    "        dataset = dataset.map(operations=trans, input_columns=\"image\", num_parallel_workers=num_worker)\n",
    "\n",
    "    # apply shuffle operations\n",
    "    dataset = dataset.shuffle(buffer_size=buffer_size)\n",
    "\n",
    "    # apply batch operations\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.repeat(repeat_num)\n",
    "\n",
    "    return dataset, count"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size is : \n",
      " 2000\n"
     ]
    }
   ],
   "source": [
    "dataset, count = data_load('Helen_192_no_enhance_do_clip', True, batch_size=1, repeat_num=1, count_number=False, num_worker=4, shuffle=None)\n",
    "print('dataset size is : \\n', dataset.get_dataset_size())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3 训练准备"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1 网络定义"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "完成数据集创建与读取以后就开始着手网络定义。FaceAlignment所用的网络由样例onnx文件包含的网络描述直接得出。这里直接引入模型定义并实例化"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "network_config = [\n",
    "    # in_channels, out_channels, kernel_size, stride, padding, dilation, group\n",
    "    [3, 16, 3, 2, 1, 1, 1],\n",
    "    [16, 16, 3, 1, 1, 1, 16],\n",
    "    [16, 32, 1, 1, 0, 1, 1],\n",
    "    [32, 32, 3, 2, 1, 1, 32],\n",
    "    [32, 64, 1, 1, 0, 1, 1],\n",
    "    [64, 64, 3, 1, 1, 1, 64],\n",
    "    [64, 64, 1, 1, 0, 1, 1],\n",
    "    [64, 64, 3, 2, 1, 1, 64],\n",
    "    [64, 128, 1, 1, 0, 1, 1],\n",
    "    [128, 128, 3, 1, 1, 1, 128],\n",
    "    [128, 128, 1, 1, 0, 1, 1],\n",
    "    [128, 128, 3, 2, 1, 1, 128],\n",
    "    [128, 256, 1, 1, 0, 1, 1],\n",
    "    [256, 256, 3, 1, 1, 1, 256],\n",
    "    [256, 256, 1, 1, 0, 1, 1],\n",
    "    [256, 256, 3, 1, 1, 1, 256],\n",
    "    [256, 256, 1, 1, 0, 1, 1],\n",
    "    [256, 256, 3, 1, 1, 1, 256],\n",
    "    [256, 256, 1, 1, 0, 1, 1],\n",
    "    [256, 256, 3, 1, 1, 1, 256],\n",
    "    [256, 256, 1, 1, 0, 1, 1],\n",
    "    [256, 256, 3, 1, 1, 1, 256],\n",
    "    [256, 256, 1, 1, 0, 1, 1],\n",
    "    [256, 256, 3, 2, 1, 1, 256],\n",
    "    [256, 512, 1, 1, 0, 1, 1],\n",
    "    [512, 512, 3, 1, 1, 1, 512],\n",
    "    [512, 512, 1, 1, 0, 1, 1],\n",
    "    [512, 64, 3, 2, 1, 1, 1]\n",
    "]\n",
    "\n",
    "class Facealignment2d(nn.Cell):\n",
    "    \"\"\"\n",
    "    Model define for 2D face alignment work\n",
    "    Model structure and layer names are directly translated from the given ONNX file\n",
    "\n",
    "    Args:\n",
    "        output_channel (int) - Should be number of alignment points * 2, this input is 388 for Helen dataset.\n",
    "\n",
    "    Inputs:\n",
    "        X(Tensor(1, 3, 192, 192)): Input image in tensor\n",
    "\n",
    "    Outputs:\n",
    "        x(Tensor(1, 1, output_channel)): Predict output. Each point takes 2 channels.\n",
    "\n",
    "    Supported Platforms:\n",
    "        ``Ascend`` ``GPU``\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_channel):\n",
    "        super(Facealignment2d, self).__init__()\n",
    "        self.network_config = network_config\n",
    "        self.features = self._make_layer(network_config, output_channel)\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"\n",
    "        Define forward pass\n",
    "        \"\"\"\n",
    "        x = self.features(x)\n",
    "        return x\n",
    "\n",
    "    def _make_layer(self, cfg: List[List[int]], output_channel: int) -> nn.SequentialCell:\n",
    "        '''\n",
    "        Make layer for model 'FaceAlignment2d'.\n",
    "\n",
    "        Args:\n",
    "            cfg: Model layer config, like 'network_config' above\n",
    "            output_channel(int) : Should be number of alignment points * 2, this input is 388 for Helen dataset.\n",
    "\n",
    "        Returns:\n",
    "            SequentialCell, Contains layers generated With 'cfg'\n",
    "\n",
    "        Examples:\n",
    "            >>>_make_layer(network_config, 388)\n",
    "        '''\n",
    "        layers = []\n",
    "        for v in cfg:\n",
    "            layers += [nn.Conv2d(in_channels=v[0], out_channels=v[1],\n",
    "                                 kernel_size=v[2], stride=v[3],\n",
    "                                 pad_mode=\"pad\",\n",
    "                                 padding=(v[4], v[4], v[4], v[4]),\n",
    "                                 dilation=v[5], group=v[6]),\n",
    "                       nn.BatchNorm2d(num_features=v[1]),\n",
    "                       nn.PReLU()]\n",
    "        out_channels = cfg[-1][1] * cfg[-1][2] * cfg[-1][2]\n",
    "        layers += [nn.Flatten(), nn.Flatten(), nn.Dense(in_channels=out_channels, out_channels=output_channel)]\n",
    "        return nn.SequentialCell(layers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Facealignment2d' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_7216\\3556662973.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mnet\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mFacealignment2d\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutput_channel\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'num_classes'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m: name 'Facealignment2d' is not defined"
     ]
    }
   ],
   "source": [
    "net = Facealignment2d(output_channel=config['num_classes'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2 损失函数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "由于任务是完成194个点（388通道，每个点的横纵坐标各对应一个通道）的回归任务，损失函数用估计点与真实点的平均曼哈顿距离即可，如下。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class MSELoss(nn.LossBase):\n",
    "    \"\"\"\n",
    "    MSELoss.\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "\n",
    "    Examples:\n",
    "        >>> MSELoss()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MSELoss, self).__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "    def construct(self, logit, label):\n",
    "        ''' Repackage MSE LOSS'''\n",
    "        x = self.mse(logit, label)\n",
    "        return x\n",
    "\n",
    "loss = MSELoss()\n",
    "loss_scale = ms.FixedLossScaleManager(\n",
    "        config['loss_scale'], drop_overflow_update=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3学习率和优化器"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "学习率变化和优化器定义如下"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def get_lr(global_step, lr_init, lr_end, lr_max, warmup_epochs, total_epochs, steps_per_epoch):\n",
    "    \"\"\"\n",
    "    Summary.\n",
    "\n",
    "    Generate learning rate array\n",
    "\n",
    "    Args:\n",
    "        global_step(int): total steps of the training\n",
    "        lr_init(float): init learning rate\n",
    "        lr_end(float): end learning rate\n",
    "        lr_max(float): max learning rate\n",
    "        warmup_epochs(int): number of warmup epochs\n",
    "        total_epochs(int): total epoch of training\n",
    "        steps_per_epoch(int): steps of one epoch, value is dataset.get_dataset_size()\n",
    "\n",
    "    Returns:\n",
    "        np.array, learning rate array\n",
    "\n",
    "    Examples:\n",
    "        >>> get_lr(0, 0, 0, 0.0001, 4, 1000, 8000)\n",
    "\n",
    "    \"\"\"\n",
    "    lr_each_step = []\n",
    "    total_steps = steps_per_epoch * total_epochs\n",
    "    warmup_steps = steps_per_epoch * warmup_epochs\n",
    "    for i in range(total_steps):\n",
    "        if i < warmup_steps:\n",
    "            lr = lr_init + (lr_max - lr_init) * i / warmup_steps\n",
    "        else:\n",
    "            lr = lr_end + \\\n",
    "                 (lr_max - lr_end) * \\\n",
    "                 (1. + math.cos(math.pi * (i - warmup_steps) / (total_steps - warmup_steps))) / 2.\n",
    "        if lr < 0.0:\n",
    "            lr = 0.0\n",
    "        lr_each_step.append(lr)\n",
    "\n",
    "    current_step = global_step\n",
    "    lr_each_step = np.array(lr_each_step).astype(np.float32)\n",
    "    learning_rate = lr_each_step[current_step:]\n",
    "\n",
    "    return learning_rate\n",
    "\n",
    "epoch_size = config['epoch_size']\n",
    "step_size = dataset.get_dataset_size()\n",
    "lr = ms.Tensor(get_lr(global_step=0, lr_init=0, lr_end=0, lr_max=config['lr'], warmup_epochs=config['warmup_epochs'], total_epochs=epoch_size, steps_per_epoch=step_size))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "优化器选择使用动量优化器"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "opt = nn.Momentum(filter(lambda x: x.requires_grad, net.get_parameters()), lr, config['momentum'], config['weight_decay'], config['loss_scale'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Monitor定义"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "class Monitor(Callback):\n",
    "    \"\"\"\n",
    "    Monitor loss and time.\n",
    "\n",
    "    Args:\n",
    "        lr_init (numpy array): train lr\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Examples:\n",
    "        >>> Monitor(100,lr_init=ms.Tensor([0.05]*100).asnumpy())\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lr_init=None):\n",
    "        super(Monitor, self).__init__()\n",
    "        self.lr_init = lr_init\n",
    "        self.lr_init_len = len(lr_init)\n",
    "\n",
    "    def epoch_begin(self, run_context):\n",
    "        \"\"\" Reset loss array and timer\"\"\"\n",
    "        self.losses = []\n",
    "        self.epoch_time = time.time()\n",
    "\n",
    "    def epoch_end(self, run_context):\n",
    "        \"\"\" Calculate epoch time and epoch average loss\"\"\"\n",
    "        cb_params = run_context.original_args()\n",
    "        epoch_mseconds = (time.time() - self.epoch_time) * 1000\n",
    "        per_step_mseconds = epoch_mseconds / cb_params.batch_num\n",
    "        print(\"epoch time: {:5.3f}, per step time: {:5.3f}, avg loss: {:5.3f}\".format(epoch_mseconds, per_step_mseconds, np.mean(self.losses)))\n",
    "\n",
    "    def step_begin(self, run_context):\n",
    "        \"\"\" Record step time\"\"\"\n",
    "        self.step_time = time.time()\n",
    "\n",
    "    def step_end(self, run_context):\n",
    "        \"\"\" Calculate step time and step average loss\"\"\"\n",
    "        cb_params = run_context.original_args()\n",
    "        step_mseconds = (time.time() - self.step_time) * 1000\n",
    "        step_loss = cb_params.net_outputs\n",
    "\n",
    "        if isinstance(step_loss, (tuple, list)) and isinstance(step_loss[0], ms.Tensor):\n",
    "            step_loss = step_loss[0]\n",
    "        if isinstance(step_loss, ms.Tensor):\n",
    "            step_loss = np.mean(step_loss.asnumpy())\n",
    "\n",
    "        self.losses.append(step_loss)\n",
    "        cur_step_in_epoch = (cb_params.cur_step_num - 1) % cb_params.batch_num\n",
    "\n",
    "        print(\"epoch: [{:3d}/{:3d}], step:[{:5d}/{:5d}], loss:[{:5.3f}/{:5.3f}], time:[{:5.3f}], lr:[{:5.3f}]\".format(\n",
    "            cb_params.cur_epoch_num -\n",
    "            1, cb_params.epoch_num, cur_step_in_epoch, cb_params.batch_num, step_loss,\n",
    "            np.mean(self.losses), step_mseconds, self.lr_init[cb_params.cur_step_num - 1]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.5模型包装"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "这一步包装好训练用的模型，定义好callback，确定ckpt保存位置"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "model = ms.Model(net, loss_fn=loss, optimizer=opt, loss_scale_manager=loss_scale)\n",
    "cb = [Monitor(lr_init=lr.asnumpy())]\n",
    "ckpt_save_dir = config['save_checkpoint_path'] + \"ckpt_\" + \"/\"\n",
    "if config['save_checkpoint']:\n",
    "    config_ck = CheckpointConfig(save_checkpoint_steps=config['save_checkpoint_epochs'] * step_size, keep_checkpoint_max=config['keep_checkpoint_max'])\n",
    "    ckpt_cb = ModelCheckpoint(prefix=\"FaceAlignment_2D\", directory=ckpt_save_dir, config=config_ck)\n",
    "    cb += [ckpt_cb]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4 开始训练"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "使用包装好的model进行训练，会以参数'save_checkpoint_epochs'为间隔保存ckpt文件于./checkpointckpt_目录下，第一次训练的命名规则为 'FaceAlignment_2D-{epoch}_{step}.ckpt'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.train(epoch_size, dataset, callbacks=cb, dataset_sink_mode=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5 评估"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "这一部分主要对训练出来的模型效果进行评估，主要的参考标准有：\n",
    "对于每一张图片，输出：\n",
    "    ION：非精度指标：双眼外侧眼角横向坐标差值，单位为像素\n",
    "    MNE：精度指标：所有预测关键点与真实关键点的距离误差，单位为ION。\n",
    "    ERR：精度指标：所有输出通道（388通道）误差之和，可以理解为所有输出点位与真实点位的曼哈顿距离总和，单位为像素。\n",
    "在所有照片评估完后，输出：\n",
    "    AUC 0.1 precision：MNE低于0.1的比例\n",
    "    AUC 0.2 precision：MNE低于0.2的比例\n",
    "    Mean Normalized Error：所有图片的预测关键点与真实关键点的距离误差经过ION归一化后的的平均值。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.1 数据读取与增强"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def dataload(mindrecord_path):\n",
    "    \"\"\"\n",
    "    Load mindrecord from File\n",
    "\n",
    "    Args:\n",
    "        mindrecord_path(string): mindrecord path\n",
    "\n",
    "    Returns:\n",
    "        Dataset Read From Path\n",
    "\n",
    "    Examples:\n",
    "        >>> dataload('/mnt/Generated.mindrecord')\n",
    "    \"\"\"\n",
    "    dataset = ds.MindDataset(mindrecord_path, columns_list=[\"image\", \"label\"])\n",
    "    count = 0\n",
    "    for _ in dataset.create_dict_iterator(output_numpy=True):\n",
    "        count += 1\n",
    "    print(\"Got {} samples in Total, Load Successful\".format(count))\n",
    "    return dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def eval_data_preprocess(dataset):\n",
    "    \"\"\"\n",
    "    Data Preprocess Function For Evaluate\n",
    "\n",
    "    Args:\n",
    "        dataset(mindrecord dataset): Loaded Dataset\n",
    "\n",
    "    Returns:\n",
    "        data_set(mindrecord dataset): Preprocessed Dataset\n",
    "    \"\"\"\n",
    "    normalize_op = ds.vision.c_transforms.Normalize(mean=[0.485 * 255, 0.456 * 255, 0.406 * 255],\n",
    "                                                    std=[0.229 * 255, 0.224 * 255, 0.225 * 255])\n",
    "    change_swap_op = ds.vision.c_transforms.HWC2CHW()\n",
    "    type_cast_op = ds.transforms.c_transforms.TypeCast(ms.float32)\n",
    "    trans = [normalize_op, change_swap_op, type_cast_op]\n",
    "    data_set = dataset.map(operations=trans, input_columns=\"image\", num_parallel_workers=1)\n",
    "    data_set = data_set.batch(batch_size=1, drop_remainder=True)\n",
    "    return data_set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.2正式评估"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "正式评估需要用到权重文件，请在pre_trained_path中填入ckpt文件路径。\n",
    "定义网络、读入数据集、加载权重文件、打包模型，对数据集中的每一张图片进行预处理以及预测并记录上述标准下的误差。\n",
    "最后整合所有误差信息并输出到屏幕。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pre_trained_path = './checkpointckpt_/FaceAlignment_2D-1000_2000.ckpt'\n",
    "channel = 388\n",
    "net = Facealignment2d(output_channel=channel)\n",
    "dataset_raw = dataload('Helen_192_no_enhance_do_clip')\n",
    "param_dict = load_checkpoint(pre_trained_path)\n",
    "load_param_into_net(net, param_dict)\n",
    "model = ms.Model(net)\n",
    "i = 0\n",
    "mnes = []\n",
    "errs = []\n",
    "for item in dataset_raw.create_dict_iterator(output_numpy=True):\n",
    "    img = []\n",
    "    img.append(item['image'].copy())\n",
    "    dataset_one = ds.GeneratorDataset(source=img, column_names=[\"image\"])\n",
    "    dataset_ready = eval_data_preprocess(dataset_one)\n",
    "    output_one = []\n",
    "    for item_one in dataset_ready.create_dict_iterator(output_numpy=True):\n",
    "        output_one = model.predict(Tensor(item_one['image']))\n",
    "    target_output = item['label'].copy().reshape((channel, 1))\n",
    "    output_np = output_one.asnumpy().reshape((channel, 1))\n",
    "    ion = np.abs(target_output[250] - target_output[290])\n",
    "    err = np.abs(target_output - output_np)\n",
    "    errs.append(np.true_divide(err, ion))\n",
    "    tmp = np.sum(err)\n",
    "    mne = np.true_divide(tmp, ion * channel)\n",
    "    mnes.append(mne)\n",
    "    print(\"Cur Img Index : \" + str(i))\n",
    "    print(\"ION : \" + str(ion))\n",
    "    print(\"MNE : \" + str(mne))\n",
    "    print(\"ERR : \" + str(tmp))\n",
    "    img[0] = img[0] * 256\n",
    "    for j in range(int(channel/2)):\n",
    "        cv2.circle(img[0], (int(output_np[j * 2]), int(output_np[j * 2 + 1])), 2, (0, 0, 255), 1)\n",
    "    cv2.imwrite('./predict/' + str(i) + '.jpg', img[0])\n",
    "    i += 1\n",
    "total_count = i * channel\n",
    "positive_1 = 0\n",
    "positive_2 = 0\n",
    "print(len(errs))\n",
    "for k in range(i):\n",
    "    for l in range(channel):\n",
    "        if errs[k][l] < 0.1:\n",
    "            positive_1 += 1\n",
    "        if errs[k][l] < 0.2:\n",
    "            positive_2 += 1\n",
    "meannormerror = np.array(mnes).sum() / i\n",
    "print(\"AUC 0.1 precision : \" + str(positive_1 / total_count))\n",
    "print(\"AUC 0.2 precision : \" + str(positive_2 / total_count))\n",
    "print(\"Mean Normalized Error : \" + str(meannormerror))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6 推理"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "推理基于Retinaface的输出结果进行，也可以独立进行。\n",
    "若进行独立推理，则要求每张输入照片尽可能只包含一个人脸，指定参数的时候指定一个文件夹就好。\n",
    "若基于retinaface的数据结果进行推理，则需要指定参数：retinaface预测产生的json文件路径以及原始图片所在文件夹"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6.1 推理准备"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "定义文件夹读取函数、数据预处理函数如下：\n",
    "    文件夹读取函数用于从文件夹中读取所有图片，指定的路径参数不要以'/'结尾。使用的时候请确保该文件夹下无非图片文件！\n",
    "    数据预处理函数则用于对输入图片做基本的归一化。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def read_dir(dir_path):\n",
    "    if dir_path[-1] == '/':\n",
    "        raise \"Do not tail with /\"\n",
    "    all_files = []\n",
    "    if os.path.isdir(dir_path):\n",
    "        file_list = os.listdir(dir_path)\n",
    "        for f in file_list:\n",
    "            f = dir_path + '/' + f\n",
    "            if os.path.isdir(f):\n",
    "                sub_files = read_dir(f)\n",
    "                # Load File Inside Child Folder\n",
    "                all_files = sub_files + all_files\n",
    "            else:\n",
    "                if os.path.splitext(f)[1] in ['.jpg', '.png', '.bmp', '.jpeg']:\n",
    "                    all_files.append(f)\n",
    "    else:\n",
    "        raise \"Error,not a dir\"\n",
    "    return all_files\n",
    "\n",
    "def data_preprocess(data_set, batch_size=1):\n",
    "    normalize_op = ds.vision.c_transforms.Normalize(mean=[0.485 * 255, 0.456 * 255, 0.406 * 255],\n",
    "                                                    std=[0.229 * 255, 0.224 * 255, 0.225 * 255])\n",
    "    change_swap_op = ds.vision.c_transforms.HWC2CHW()\n",
    "    type_cast_op = ds.transforms.c_transforms.TypeCast(ms.float32)\n",
    "    trans = [normalize_op, change_swap_op, type_cast_op]\n",
    "    data_set = data_set.map(operations=trans, input_columns=\"image\", num_parallel_workers=1)\n",
    "    data_set = data_set.batch(batch_size, drop_remainder=True)\n",
    "    return data_set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6.2 独立推理"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "独立推理只是将图片打包使用模型预测，不涉及准备阶段提到的json和裁剪问题。\n",
    "这里指定好目标文件夹以及预训练模型的路径，执行后会将照片和标记输出到源文件夹下。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "image_dir = './images/facealignment/infer'\n",
    "pre_trained = './checkpointckpt_/FaceAlignment_2D-2150_2000.ckpt'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def infer(image_dir, pre_trained):\n",
    "    imgs = read_dir(image_dir)\n",
    "    net = Facealignment2d(output_channel=388)\n",
    "    param_dict = load_checkpoint(pre_trained)\n",
    "    load_param_into_net(net, param_dict)\n",
    "    model = ms.Model(net)\n",
    "    for file in imgs:\n",
    "        image = cv2.imread(file)\n",
    "        image = np.array(image)\n",
    "        image = cv2.resize(image, (192, 192))\n",
    "        raw_image = image.copy()\n",
    "        image = image/255\n",
    "        imgs = []\n",
    "        imgs.append(image.copy())\n",
    "        dataset_one = ms.dataset.GeneratorDataset(source=imgs, column_names=[\"image\"])\n",
    "        dataset = data_preprocess(dataset_one, batch_size=1)\n",
    "\n",
    "\n",
    "        for item_one in dataset.create_dict_iterator(output_numpy=True):\n",
    "            output_one = model.predict(ms.Tensor(item_one['image']))\n",
    "            result = np.array(output_one).astype(int).reshape((194, 2))\n",
    "            np.savetxt(file+\"_predict\", result, delimiter=\",\")\n",
    "\n",
    "            for i in range(194):\n",
    "                raw_image = cv2.circle(raw_image, (int(result[i, 0]), int(result[i, 1])), 2, (0, 0, 255), 1)\n",
    "            cv2.imwrite(file+\"_predict.jpg\", raw_image)\n",
    "\n",
    "infer(image_dir, pre_trained)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6.3 联合推理"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "这一步将会对图片（包含多张人脸的图片）使用retinaface识别输出的json文件进行裁剪。会先输出裁剪结果，再进行常规的推理。\n",
    "需要定义的有：json文件解析函数、图片裁剪函数。\n",
    "    1.json文件解析函数用于解析retinaface输出的json文件，包含对图片中各个人脸位置的表达。在解析后会输出裁剪后的人脸到文件夹\n",
    "    2.图片裁剪函数用于从原图中裁剪出各个人脸。该函数会处理超过边界的人脸框。\n",
    "在这一步处理的时候请确保目录结构如下\n",
    "\n",
    "```text\n",
    "├── （输入的路径）/\n",
    "    ├── infer（原图路径）/\n",
    "        ├── 1.jpg\n",
    "        ├── 2.jpg\n",
    "        ├── 3.jpg\n",
    "        ├── ...\n",
    "    ├── infer.json\n",
    "    ├── single（如果没有，请新建这个文件夹）/\n",
    "```\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def resolve_json(json_path):\n",
    "    json_file = open(json_path + '/infer.json', 'r', encoding='utf-8')\n",
    "    description = json.load(json_file)\n",
    "    counter = 0\n",
    "    for x in range(len(description)):\n",
    "\n",
    "        # For each Picture\n",
    "        temp_key = list(description.keys())[x]\n",
    "        img = description[temp_key]\n",
    "        img_path = img['img_path']\n",
    "        read_img = cv2.imread(json_path+\"/\"+img_path)\n",
    "        bboxes = img['bboxes']\n",
    "\n",
    "        for i in range(len(bboxes)):\n",
    "            if bboxes[i][4] > 0.95:\n",
    "                # For Each Face\n",
    "                img_clipped = pic_clip(read_img, bboxes[i][0], bboxes[i][1], bboxes[i][2], bboxes[i][3])\n",
    "                img_resized = cv2.resize(img_clipped, (192, 192))\n",
    "                cv2.imwrite(json_path+'/single/' + str(counter) + \".jpg\", img_resized)\n",
    "                counter += 1\n",
    "\n",
    "def pic_clip(img, x, y, width, height):\n",
    "    if x < 0:\n",
    "        t0 = 0\n",
    "    else:\n",
    "        t0 = x\n",
    "    if y < 0:\n",
    "        t1 = 0\n",
    "    else:\n",
    "        t1 = y\n",
    "    if x + width < img.shape[1]:\n",
    "        t2 = x + width\n",
    "    else:\n",
    "        t2 = img.shape[1]\n",
    "    if y + height < img.shape[0]:\n",
    "        t3 = y + height\n",
    "    else:\n",
    "        t3 = img.shape[0]\n",
    "    img_clipped = img[int(t1):int(t3), int(t0):int(t2)]\n",
    "    return img_clipped"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "path = '/mnt/c/Users/27976/Documents/WeChat Files/wxid_kzy8nz8xw3ug22/FileStorage/MsgAttach/4a643c27dd06f319f9721630b8d045e7/File/2022-07/infer'\n",
    "resolve_json(path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "之后的内容相当于正常的独立推理"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "infer(path+'/single', pre_trained)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7 推理效果"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![0.jpeg_predict.jpg](./images/facealignment/infer/0.jpeg_predict.jpg)\n",
    "![1.jpeg_predict.jpg](./images/facealignment/infer/1.jpeg_predict.jpg)\n",
    "![2.jpeg_predict.jpg](./images/facealignment/infer/2.jpeg_predict.jpg)\n",
    "![3.jpeg_predict.jpg](./images/facealignment/infer/3.jpeg_predict.jpg)\n",
    "![4.jpeg_predict.jpg](./images/facealignment/infer/4.jpeg_predict.jpg)\n",
    "![5.jpeg_predict.jpg](./images/facealignment/infer/5.jpeg_predict.jpg)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}